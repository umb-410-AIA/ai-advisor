#!/usr/bin/env python3
# Code generated by Codex (GPT-5) in response to user prompt:
# "Write multiple scripts that will take the different University cataog files in the data folder as an input and then output the data within those files into a new json file with the following schema..."

from __future__ import annotations

import argparse
import json
import re
from pathlib import Path
from typing import List, Sequence

COMMON_REQUIREMENT_LABEL = r'(?:pre|co)(?:\s*re)?[-\s]?req(?:uisite)?s?'


def _build_requirement_pattern(prefix: str) -> re.Pattern:
    label = rf'(?:{prefix})(?:\s*re)?[-\s]?req(?:uisite)?s?'
    return re.compile(
        rf'({label}(?:\s*[:=\-])?\s*)(.*?)(?=(?:[.;,\)]?\s*(?<!\bor\s){COMMON_REQUIREMENT_LABEL}\b)|$)',
        re.IGNORECASE | re.DOTALL,
    )


PREREQ_PATTERN = _build_requirement_pattern("pre")
COREQ_PATTERN = _build_requirement_pattern("co")

COURSE_CODE_PATTERNS = (
    re.compile(r'\b\d{1,3}\.(?:\d{2,3}|[A-Z]\d{2})[A-Z]?\b', re.IGNORECASE),
    re.compile(
        r'\b(?:(?=[A-Z0-9]*[A-Z])[A-Z0-9]{1,6})(?:-(?=[A-Z0-9]*[A-Z])[A-Z0-9]{1,6})*[\s\.-]*\d{1,4}(?:\.\d+)?[A-Z]?\b',
        re.IGNORECASE,
    ),
)

LEADING_CONNECTOR_PATTERN = re.compile(r'^(?:AND|OR|AND/OR|EITHER)\s+', re.IGNORECASE)
TRAILING_CONNECTOR_PATTERN = re.compile(r'\s+(?:AND|OR|AND/OR)$', re.IGNORECASE)
PREFIX_STOPWORDS = {
    "AND",
    "OR",
    "AND/OR",
    "MORE",
    "LESS",
    "THAN",
    "WITH",
    "PLUS",
    "MINUS",
    "OVER",
    "UNDER",
    "TO",
    "OF",
    "FOR",
    "BY",
    "THE",
    "A",
    "AN",
    "ANY",
    "ALL",
    "EITHER",
    "BOTH",
}


def extract_course_ids(*texts: str | None) -> List[str]:
    seen: set[str] = set()
    codes: List[str] = []
    for text in texts:
        if not text:
            continue
        normalized = clean_text(text)
        if not normalized:
            continue
        for pattern in COURSE_CODE_PATTERNS:
            for match in pattern.finditer(normalized):
                code = match.group(0)
                code = re.sub(r'[\[\]\(\)]', '', code)
                code = re.sub(r'\s+', ' ', code.strip(" ,.;:"))
                code = LEADING_CONNECTOR_PATTERN.sub("", code)
                code = TRAILING_CONNECTOR_PATTERN.sub("", code)
                code = code.upper()
                if not code:
                    continue
                first_token = code.split(" ", 1)[0]
                if first_token in PREFIX_STOPWORDS:
                    continue
                if code and code not in seen:
                    seen.add(code)
                    codes.append(code)
    return codes



DEFAULT_NOTE_KEYWORDS = (
    "permission",
    "consent",
    "coreq",
    "corequisite",
    "recommended",
    "majors only",
    "limited to",
    "jointly",
    "cannot be applied",
    "REST",
    "GIR",
)


def normalize_path(path_str: str) -> Path:
    return Path(path_str).expanduser().resolve()


def load_json(path: Path):
    with path.open("r", encoding="utf-8") as infile:
        return json.load(infile)


def write_json(path: Path, payload) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", encoding="utf-8") as outfile:
        json.dump(payload, outfile, indent=2, ensure_ascii=False)


def clean_text(value: str | None) -> str:
    if not value:
        return ""
    text = re.sub(r"[\u00a0]", " ", value)
    text = re.sub(r"\s+", " ", text)
    return text.strip()


def infer_course_id(raw_name: str | None) -> str:
    if not raw_name:
        return ""
    cleaned = clean_text(raw_name)
    cleaned = cleaned.lstrip("* ,")
    if not cleaned:
        return ""
    tokens = re.split(r"[\s,]+", cleaned)
    if not tokens:
        return ""

    first = tokens[0].strip(",")
    second = tokens[1].strip(",") if len(tokens) > 1 else ""
    if first.isalpha() and second and re.match(r"^[\dA-Z]", second):
        candidate = f"{first} {second}"
    else:
        candidate = first
    return candidate


def _extract_statements(
    pattern: re.Pattern,
    texts: Sequence[str | None],
) -> List[str]:
    collected: List[str] = []
    for text in texts:
        if not text:
            continue
        for match in pattern.finditer(text):
            snippet = "".join(match.groups())
            cleaned = clean_text(snippet)
            if cleaned and cleaned not in collected:
                collected.append(cleaned)
    return collected


def extract_prereq_statements(*texts: str | None) -> List[str]:
    return _extract_statements(PREREQ_PATTERN, texts)


def extract_coreq_statements(*texts: str | None) -> List[str]:
    return _extract_statements(COREQ_PATTERN, texts)


def extract_noteworthy_sentences(
    text: str | None,
    keywords: Sequence[str] | None = None,
) -> List[str]:
    if not text:
        return []
    search_terms = tuple(k.lower() for k in (keywords or DEFAULT_NOTE_KEYWORDS))
    sentences = re.split(r"(?<=[.!?])\s+|\n+", text)
    notes: List[str] = []
    for sentence in sentences:
        cleaned = clean_text(sentence)
        if not cleaned:
            continue
        lowered = cleaned.lower()
        if any(term in lowered for term in search_terms) and cleaned not in notes:
            notes.append(cleaned)
    return notes


def build_arg_parser(default_input: str, default_output: str) -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description=(
            "Convert a raw university course catalog JSON export into the "
            "normalized course schema."
        )
    )
    parser.add_argument(
        "--input",
        default=default_input,
        help="Path to the raw course catalog JSON file.",
    )
    parser.add_argument(
        "--output",
        default=default_output,
        help="Destination path for the normalized course catalog JSON.",
    )
    return parser


def coalesce_statements(statements: Sequence[str]) -> str | None:
    if not statements:
        return None
    text = "; ".join(statements)
    return text or None
